# ğŸ¦– RAGosaurus

**Thank you for stalking :)**

> _â€œWhy build another RAG pipeline?â€_  
> Because none of them let you *just point at a GitHub repo and start asking questions like it's Stack Overflow.*  

## ğŸš¨ Problem?

No plug-and-play RAG pipeline that eats codebases and spits out coherent answers â€” unless you want to deploy LLaMA on a $30,000 GPU cluster.  

## âœ… Solution?

### ğŸ¦– **RAGosaurus** â€” a lightweight but functional notebook implementation where you:
1. Point it at a folder with Python code, and  
2. Magically start asking questions like â€œhow does `read_csv()` work?â€  

### Pros?
- You donâ€™t need an RTX 4090.  
- Works on dusty GPUs with just **6GB VRAM** (tested on 1660 Ti).  
- Actually generates useful answers.  
- No over-engineered LangChain spaghetti.
- SBERT + FAISS options for retrieval â€” because variety is the spice of life.

---

## ğŸ”§ How It Works

- ğŸ§  **Encoders**
- CodeT5: Salesforce/codet5-base for code-aware embeddings.
- SBERT (optional): all-MiniLM-L6-v2 for semantic code/document matching.
- Custom pooling: Mean-pooled last hidden states with attention mask weighting for more stable embeddings.  
- ğŸ“¦ **Retriever:** Retriever
  -Flat L2 index for fast similarity search, persisted to disk.
  -SBERT pipeline: Cosine similarity search over precomputed normalized embeddings.
- âœï¸ **Generator:** [`microsoft/phi-2`](https://huggingface.co/microsoft/phi-2) generates the final answers.

Yes, we handle token truncation and all that windowing magic behind the scenes.

### Chunking & Indexing
RAGosaurus uses token-aware overlapping chunking to preserve context in code.
- **Tokenizer**: Salesforce/codet5-base
- **Max tokens**: 500 per chunk, stride 100
- **Overlap**: Ensures function/class definitions arenâ€™t split mid-body.
- **Caching**: Chunks and embeddings are cached to disk for instant reuse.
- **Batch embeddings**: 32-chunk batches to minimize GPU memory usage.
- **Indexing**: FAISS flat L2 index for fast retrieval, persisted to disk.

This design keeps the pipeline fast (<X ms retrieval on 6 GB GPU) while maintaining high retrieval relevance.


---

## ğŸ¤– Example Usage

```python
query = "How does pandas read a CSV file?"
query_emb = embed_query(query, embed_tokenizer, embed_model)

# FAISS retrieval
chunks = retrieve_top_k(query_emb, index, code_chunks, k=5)

# Generate answer
answer = generate_answer(query, chunks, gen_tokenizer, gen_pipe)
print(answer)

```

---

# ğŸ§ª What You Can Play With
- Swap FAISS with SBERT if you're into semantic vibes

- Adjust top_k retrieved chunks (2, 5, 10, 420?)

- Tweak prompt templates or more verbose or terse answers.

- Compare baseline vs.reranked retrieval quality.

---

# ğŸ” Smart-Sounding Observations
SBERT retrieval can pull in semantically richer chunks, but sometimes overshoots.

FAISS + CodeT5 embeddings excel at syntax-aware matches.

Fetching too many chunks (k > 10) turns output into token soup.

Phi-2 is surprisingly good for technical queries â€” just feed it concise context.

---

## ğŸ› ï¸ Yes, It Uses LangChain â€” But Not in the Way That Sucks

Youâ€™ll see LangChain imports â€” and thatâ€™s fine.  
But this ainâ€™t your typical bloated LangChain pipeline:

- âœ… Used **only** for simple wrappers like retrievers, prompt templates, and huggingface pipelines
- âŒ No agents, tool runners, chain nesting madness
- ğŸ§  Most of the logic (like chunking, encoding, inference) is **manually wired**
- âš’ï¸ Think: LangChain *as a toolbox*, not a religion
---

## ğŸ§‘â€ğŸ’» Setup

git clone https://github.com/p-doshi/RAGosaurus.git

cd RAGosaurus

---

## ğŸ“¦ Requirements
- Python 3.10+
- PyTorch (with or without CUDA)
- transformers
- sentence-transformers
- faiss
- tqdm

GPU optional (but helps)



GPU optional, but helpful
---

## âœï¸ Citation (Not That Anyone Ever Does)
Doshi, P. (2025). RAGosaurus: Retrieval-Augmented Generation for Codebases. GitHub.
---

## ğŸ§ƒ P.S.
Still a WIP â€” PRs, stars, or salty issues are welcome.
Also, if you improve this, Iâ€™m definitely stealing your code. Cheers.
