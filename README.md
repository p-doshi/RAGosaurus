# ğŸ¦– RAGosaurus

**Thank you for stalking :)**

> _â€œWhy build another RAG pipeline?â€_  
> Because none of them let you *just point at a GitHub repo and start asking questions like it's Stack Overflow.*  

## ğŸš¨ Problem?

No plug-and-play RAG pipeline that eats codebases and spits out coherent answers â€” unless you want to deploy LLaMA on a $30,000 GPU cluster.  

## âœ… Solution?

### ğŸ¦– **RAGosaurus** â€” a janky but functional notebook implementation where you:
1. Point it at a folder with Python code, and  
2. Magically start asking questions like â€œhow does `read_csv()` work?â€  

### Pros?
- You donâ€™t need an RTX 4090.  
- Works on dusty GPUs with just **6GB VRAM** (tested on 1660 Ti).  
- Actually generates useful answers.  
- No over-engineered LangChain spaghetti.  

---

## ğŸ”§ How It Works

- ğŸ§  **Encoder:** [`Salesforce/codet5-base`](https://huggingface.co/Salesforce/codet5-base) handles code chunk embeddings.  
- ğŸ“¦ **Retriever:** FAISS does vector search. You can also try SBERT.  
- âœï¸ **Generator:** [`microsoft/phi-2`](https://huggingface.co/microsoft/phi-2) generates the final answers.

Yes, we handle token truncation and all that windowing magic behind the scenes.



---

## ğŸ¤– Example Usage

```python
query = "How does pandas read a CSV file?"
query_emb = embed_query(query, embed_tokenizer, embed_model)

# FAISS retrieval
chunks = retrieve_top_k(query_emb, index, code_chunks, k=5)

# Generate answer
answer = generate_answer(query, chunks, gen_tokenizer, gen_pipe)
print(answer)

```

---

# ğŸ§ª What You Can Play With
Swap FAISS with SBERT if you're into semantic vibes

Adjust top_k retrieved chunks (2, 5, 10, 420?)

Tweak prompt templates

Compare baseline vs. tuned generation

---

# ğŸ” Smart-Sounding Observations
SBERT gives more semantically aligned chunks than FAISS (but slower).

Prompt tweaks dramatically affect output quality.

Fetching too many chunks (k > 10) turns answers into token soup.

Phi-2 does well with technical queries â€” just donâ€™t overwhelm it.

---

## ğŸ› ï¸ Yes, It Uses LangChain â€” But Not in the Way That Sucks

Youâ€™ll see LangChain imports â€” and thatâ€™s fine.  
But this ainâ€™t your typical bloated LangChain pipeline:

- âœ… Used **only** for simple wrappers like retrievers, prompt templates, and huggingface pipelines
- âŒ No agents, tool runners, chain nesting madness
- ğŸ§  Most of the logic (like chunking, encoding, inference) is **manually wired**
- âš’ï¸ Think: LangChain *as a toolbox*, not a religion
---

## ğŸ§‘â€ğŸ’» Setup

git clone https://github.com/p-doshi/RAGosaurus.git

cd RAGosaurus

---

## ğŸ“¦ Requirements
Python 3.10+

PyTorch (w/ or w/o CUDA)

transformers, sentence-transformers, faiss, tqdm, etc.

GPU optional, but helpful
---

## âœï¸ Citation (Not That Anyone Ever Does)
Doshi, P. (2025). RAGosaurus: Retrieval-Augmented Generation for Codebases. GitHub.
---

## ğŸ§ƒ P.S.
Still a WIP â€” PRs, stars, or salty issues are welcome.
Also, if you improve this, Iâ€™m definitely stealing your code. Cheers.
